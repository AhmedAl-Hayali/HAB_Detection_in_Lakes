{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2PX3_Final_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedAl-Hayali/HAB_Detection_in_Lakes/blob/main/2PX3_Final_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lot of the code is self-authored and adapted from previous independent projects hence I have so few references. If anything, I may have modified some code from Eli Stevens, Luca Antiga, and Thomas Viehmann's \"Deep Learning with PyTorch\" in previous iterations of the code used here."
      ],
      "metadata": {
        "id": "GdbeVA_EoW_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & Formatting\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "from PIL import Image, ImageEnhance\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import datetime"
      ],
      "metadata": {
        "id": "ccPdQxXnEKdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing training data\n",
        "from zipfile import ZipFile\n",
        "file_name = \"2px3_wk7_imgs.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTP6v3F7IVTN",
        "outputId": "751853fb-3c19-40d3-f39c-e9aaf6d8dbd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting all operations to operate on the GPU for better performance\n",
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "          else torch.device('cpu'))\n",
        "print(f\"Training on device {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSpF1re1Wnv3",
        "outputId": "3804cd88-abb2-4df9-d0cd-ca744b74600e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting training image data; credit: 2PX3 Wk 5 code\n",
        "\n",
        "labels = ['alge', 'no_alge']\n",
        "img_size = 224\n",
        "\n",
        "def get_data():\n",
        "    # Pre-instantiate empty lists to store data and corresponding labels\n",
        "    data = [] \n",
        "    data_labels = []\n",
        "\n",
        "    # Iterating over labels\n",
        "    for label in labels: \n",
        "        path = label + \"_imgs\"\n",
        "        class_num = labels.index(label)\n",
        "        # Iterating over images for each label\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                # Reading and storing images in an np array\n",
        "                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
        "                data.append(resized_arr)\n",
        "                data_labels.append(class_num)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        x = np.array(data)\n",
        "        y = np.array(data_labels)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "JM0Kig0JIZgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing images and corresponding labels in np arrays\n",
        "data, labs = get_data()"
      ],
      "metadata": {
        "id": "8VTTQx9eTY5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing training data\n",
        "data_mod = torch.tensor(data, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "# Computing mean and standard deviation\n",
        "data_sd, data_mean = torch.std_mean(data_mod, dim=(0,2, 3)) # Collapse spatial and batch dims\n",
        "# tensor([72.3099, 65.2078, 67.4276]) tensor([112.5567, 120.7564, 108.9186])\n",
        "\n",
        "# Normalizing data\n",
        "data_normal = transforms.functional.normalize(data_mod, mean=data_mean, std=data_sd)"
      ],
      "metadata": {
        "id": "TdEzMi1jPmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting and preprocessing validation data\n",
        "def get_img(link):\n",
        "    try:\n",
        "        img_html = requests.get(link)\n",
        "        img = Image.open(BytesIO(img_html.content))\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    \n",
        "    return img\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "                                 transforms.CenterCrop([224, 224]), # scale input to 224 x 224\n",
        "                                 transforms.ToTensor(), # transform to a tensor, 3D array w/ color & 2 spatial dims\n",
        "                                 transforms.Normalize([112.5567, 120.7564, 108.9186], [72.3099, 65.2078, 67.4276])\n",
        "                                 ])\n",
        "\n",
        "# Links for images of algal-bloom water and water without an algal bloom\n",
        "alge_tests_links = [\"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200131-05-135x104.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200131-07-135x114.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200131-08-135x90.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200131-04-135x108.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-1-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200131-09-135x127.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-4-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-4-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-5-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200131-03-135x90.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-2-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200727-01-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200727-03-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-6-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-7-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200131-01-135x91.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200727-04-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200727-02-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20200131-02-135x90.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-9-thumbnail.jpg\", \"https://www.canada.ca/content/dam/eccc/images/eolakewatch/20210712-3-thumbnail.jpg\"]\n",
        "\n",
        "noalge_tests_links = [\"https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/iss036e035632.jpg\", \"https://images.fineartamerica.com/images-medium-large-5/1-satellite-view-of-crater-lake-oregon-panoramic-images.jpg\", \"https://content.satimagingcorp.com/static/galleryimages/ikonos-atsukeshi-lake-japan.jpg\", \"https://www.overv.eu/wp-content/uploads/2017/11/lake-victoria.jpg\", \"https://eoportal.org/documents/163813/248818/Lake-st-Clair-Deimos-1\", \"https://www.fws.gov/sites/default/files/styles/scale_width_1200/public/2020-10/LakeOntario_NOAA_publicdomain.jpg?itok=UbAMoCle\", \"https://static.eos.com/wp-content/uploads/2021/11/Lake-Ontario.jpg\", \"https://www.robertharding.com/watermark.php?type=preview&im=RM/RH/HORIZONTAL/1348-3045\", \"https://live.staticflickr.com/7544/15869976110_4c7ee8da95_b.jpg\"]"
      ],
      "metadata": {
        "id": "By6XkY_YfsIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting the data into a list of tuples, each tuple containing the tensor of the image and its corresponding label (0 alge, 1 no alge)\n",
        "\n",
        "# Training data\n",
        "data_norm_lab = [(d, torch.tensor(labs[i], dtype=torch.float)) for i, d in enumerate(data_normal)]\n",
        "\n",
        "# Validation data; This will take some time because it gets the image from the internet so we deal with data traffic delay :/\n",
        "val_data = [(preprocess(get_img(l)), 0) for l in alge_tests_links] + [(preprocess(get_img(l)), 1) for l in noalge_tests_links]"
      ],
      "metadata": {
        "id": "4xq8bx_YWAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data loader (for batching 16 images at a time); Comment out when not training\n",
        "train_loader = torch.utils.data.DataLoader(data_norm_lab, batch_size=16,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# Validation data loader (for batching 16 images at a time); Comment out when not validating\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=16,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# Function to validate results after training\n",
        "def validate(model, train_loader, val_loader):\n",
        "    # Instantiating empty dict for separating train and val accuracies\n",
        "    accdict = {}\n",
        "    # Iterating through loaders (train & val)\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        # Correct & total predictions tally\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Operating without gradients because this is evaluation not training\n",
        "        with torch.no_grad():\n",
        "            # Iterating through images and corresponding labels in current loader\n",
        "            for imgs, labels in loader:\n",
        "                # Ensuring images and labels are on the same device (cpu or gpu)\n",
        "                imgs = imgs.to(device=device)\n",
        "                labels = labels.to(device=device)\n",
        "\n",
        "                # Model output for proability of image being each class\n",
        "                outputs = model(imgs)\n",
        "                # print(outputs)\n",
        "                # Highest-probability class\n",
        "                _, predicted = torch.max(outputs, dim=-1)\n",
        "\n",
        "                # Updating total & correct tallies\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "        \n",
        "        # Outputting accuracy for specified loader\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "        # Caching accuracy result for specified loader in dict\n",
        "        accdict[name] = correct / total\n",
        "    return accdict"
      ],
      "metadata": {
        "id": "EOlPyF71IOn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
        "                        train_loader):\n",
        "    # Traversing dataset for n_epochs times and updating parameters as necessary.\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Tracking (train) loss across individual epochs\n",
        "        loss_train = 0.0\n",
        "        # Traversing images and corresponding labels in train data loader\n",
        "        for imgs, labls in train_loader:\n",
        "            imgs = imgs.to(device=device)\n",
        "            labls = labls.to(device=device, dtype=torch.long)\n",
        "\n",
        "            # Model predictions for each image\n",
        "            outputs = model(torch.squeeze(imgs))\n",
        "            # print(outputs)\n",
        "            \n",
        "            # Computing loss and regularizing (l2 reg) it\n",
        "            loss = loss_fn(outputs, labls)\n",
        "            l2_lambda = 0.001\n",
        "            l2_norm = sum(p.pow(2.0).sum()\n",
        "                          for p in model.parameters())\n",
        "            loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "            # Backpropagating after clearing gradient to prevent gradient accumulation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Updating loss\n",
        "            loss_train += loss.item()\n",
        "        # Updating client every 10 epochs of current epoch and training loss\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "OodS73DMLPOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a 7x7 kernel and 3x3 padding (0-padding to be specific)\n",
        "k_size = (7, 7)\n",
        "pad = (3, 3)\n",
        "\n",
        "class NetRes(nn.Module):\n",
        "    # Net architecture\n",
        "    def __init__(self, n_chans1=32, img_size=224):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.img_size = img_size\n",
        "        # 1st conv layer - C : 3 -> n_chans1, K: 7x7 kernel, P: 3x3 0-padding\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=k_size, padding=pad)\n",
        "        # 2nd conv layer - C : n_chans1 -> n_chans1 // 2, K: 7x7 kernel, P: 3x3\n",
        "        # 0-padding\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=k_size,\n",
        "                               padding=pad)\n",
        "        # 1st conv layer - C : n_chans1 // 2 -> n_chans1 // 2, K: 7x7 kernel,\n",
        "        # P: 3x3 0-padding\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
        "                               kernel_size=k_size, padding=pad)\n",
        "        # 1st fully conn layer - collapse from (img_size/8)^2 * n_chans1 // 2\n",
        "        # to 32\n",
        "        self.fc1 = nn.Linear((self.img_size >> 3)**2 * self.n_chans1 // 2, 32)\n",
        "        # 2nd fully conn layer - collapse from 32 to 2\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Forward pass - pooling after every conv layer, with a residual at\n",
        "        # the 3rd\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        out1 = out\n",
        "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
        "        # Reshaping from NxCxHxW tensor to Nx(C*H*W) matrix for dimension\n",
        "        # compatibility\n",
        "        out = out.view(-1, (self.img_size >> 3)**2 * self.n_chans1 // 2)\n",
        "        # Relu after first fully conn layer\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        # Relu after 2nd fully conn layer + reshaping for dimension\n",
        "        # compatibility\n",
        "        out = torch.relu(torch.squeeze(self.fc2(out)))\n",
        "        return out\n",
        "\n",
        "# Instantiating model to the net architecture above, and setting device\n",
        "model = NetRes(n_chans1=16).to(device=device)\n",
        "# Setting optimizer to AdamW (refer to https://arxiv.org/pdf/1711.05101.pdf for extreme detail; I just use it because it's advised)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
        "# Binary cross entropy loss - there are better options, but it doesn't matter for this project\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "7XHcEf1CLeMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling training loop to train model; Comment out when not training\n",
        "model.train()\n",
        "training_loop_l2reg(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "F5DnB96KLf1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93047c7c-a064-435b-d7fc-a5cc36721b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-04-14 22:10:51.117251 Epoch 1, Training loss 3.0362828969955444\n",
            "2022-04-14 22:10:53.201709 Epoch 10, Training loss 2.7107034623622894\n",
            "2022-04-14 22:10:55.514215 Epoch 20, Training loss 2.423653483390808\n",
            "2022-04-14 22:10:57.833788 Epoch 30, Training loss 2.1904627084732056\n",
            "2022-04-14 22:11:00.164415 Epoch 40, Training loss 1.9970385432243347\n",
            "2022-04-14 22:11:02.489525 Epoch 50, Training loss 1.8345318585634232\n",
            "2022-04-14 22:11:04.824346 Epoch 60, Training loss 1.6968109160661697\n",
            "2022-04-14 22:11:07.151945 Epoch 70, Training loss 1.579355463385582\n",
            "2022-04-14 22:11:09.477256 Epoch 80, Training loss 1.4786954522132874\n",
            "2022-04-14 22:11:11.800156 Epoch 90, Training loss 1.392090991139412\n",
            "2022-04-14 22:11:14.128669 Epoch 100, Training loss 1.3173342943191528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model after training; comment out if not training\n",
        "# torch.save(model.state_dict(), f=\"2px3_final_model\")\n",
        "\n",
        "# Loading model we saved earlier\n",
        "model.load_state_dict(torch.load(\"/content/2px3_final_model\"))\n",
        "# Setting model to eval mode (refer to https://stackoverflow.com/a/60018731 for details on why we do this)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "vJzcHhCfFKxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f78c35-37a9-4838-a111-5980d4b7e304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetRes(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "  (conv2): Conv2d(16, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "  (conv3): Conv2d(8, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "  (fc1): Linear(in_features=6272, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transferring model to GPU then validating\n",
        "model.cuda()\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "_UVtv1bmVZFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5a7977-b652-4d74-b6c3-a15114e4ce91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.99\n",
            "Accuracy val: 0.70\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 0.9919354838709677, 'val': 0.7}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}